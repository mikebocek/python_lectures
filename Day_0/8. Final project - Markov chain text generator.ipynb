{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "import unittest\n",
    "import reference_implementation as ref_impl # Solutions to the problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congrats! ðŸŽ‰\n",
    "\n",
    "Believe it or not, you've learned a huge chunk of the Python language, and have all the basic skills you need to move on to learning the scientific libraries and start playing with data! We'll start doing this in the next set of excercises. There's one chapter after this one that you should complete, which will deal with some more advanced language features that are extremely useful, and make Python a more fun and expressive language to code in. For now though, you should feel accomplished that you've made it this far!\n",
    "\n",
    "The concepts and skills that you've learned (the if statement, for loops, lists, dictionaries, strings) are pretty universal among programming languages. If you wanted to try web programming in JavaScript, or write Desktop applications in C++, you'll find these same concepts come up.\n",
    "\n",
    "What we're gonna do now is try to put together all of the basic Python you've learned into one coherent project. This way, you can get a feel for how all of these parts work together beyond the single-function excercises that you've been doing.\n",
    "\n",
    "# Our project\n",
    "\n",
    "What we're going to create is a Markov-Chain text generator. In effect, what this program is going to do is read a file with sentences from a particular source (a novel, song lyrics, speeches, etc.) and then generate random, usually nonsensical text in the style of this source material. The algorithm to do this is incredibly simple, but these programs can sometimes create quite sophisticated looking sentences! \n",
    "\n",
    "If you want an example of what Markov Chains can do, the website Reddit has a community (http://www.reddit.com/r/SubredditSimulator) where the title of every post, as well as every comment on the posts, are generated randomly from the text of other communities on Reddit. The results are often pretty funny, and sometimes seem surprisingly intelligent.\n",
    "\n",
    "# Some examples\n",
    "\n",
    "Since this is a much more difficult challenge than any of the previous excercises, I've provided a Python library with (working) versions of all the functions you'll have to implement in the challenge. Let's use library to produce some model Markov-chain generated sentences. First, we'll load two text corpuses into two data structures. In this case, the corpuses are drawn from speeches given by Barack Obama and Donald Trump before they assumed the presidency. For this, we'll use the word_counts_from_file() function in the library. In this notebook, you'll implement this function and the various helper functions it uses on your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trump_counts = ref_impl.word_counts_from_file('text_corpuses/trump_speeches.txt')\n",
    "obama_counts = ref_impl.word_counts_from_file('text_corpuses/obama_speeches.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using these data structures, we'll create some mock sentences using the patterns our program has \"learned\" from these texts. For this, we'll use the generate_sentence() function. You will also implement this at the end of the excercise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CNN â€“ I said Iâ€™ll tell you. Iâ€™m not my life. If anyone who Iâ€™m not being funded by the right thing. We have to explode. In fact that will tell you people we take a dumping ground for America, but have everything. They were, during one of our tunnels,'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_impl.generate_sentence(trump_counts, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"war that the powers it and we're doing this Chamber right folks. There are Iraqis to anybody on the Rwandan genocide that we may not us? Who said by men and venture capital to find a more than the fact that the Foreign Minister of you were tough times. But\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_impl.generate_sentence(obama_counts, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run these functions again, you'll get a different sentence. As you'll notice, the sentences don't make a lot of sense and sometimes don't follow the rules of grammar very well. But they somehow work surprisingly well at capturing the tone of the speaker!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do Markov chain text generators work?\n",
    "\n",
    "The basic idea behind Markov chain text generation is simple. Essentially, we first pick a word at random from the document that we've picked to generate our text. Let's say we pick the word \"dinosaurs.\" We then find every word that follows the word \"dinosaurs\" in the document. In this case, we might see the phrases:\n",
    "\n",
    "- \"dinosaurs lived millions of years ago\"\n",
    "- \"dinosaurs lived in a very different world\"\n",
    "- \"dinosaurs gave rise to birds\"\n",
    "\n",
    "and so we find that the word dinosaurs is followed twice by the word \"lived\" and once by the word \"gave.\" We would randomly chose the next word as either \"lived\" (with a 2/3 probability) or \"gave\" (with a 1/3) probability. Let's say we picked \"gave.\" We might have the sentences \n",
    "\n",
    "- \"gave rise to birds\"\n",
    "- \"gave me an expensive gift\"\n",
    "- \"gave me a black eye\"\n",
    "- \"gave this old-timer another shot\"\n",
    "\n",
    "And so we would now pick between \"rise\", \"me\" and \"this\" as the next word that we pick. After picking one of these words, we would continue the chain until we've generated a pre-determined amount of text.\n",
    "\n",
    "It would be very computationally expensive to re-scan the document every time that we picked a new word in order to find out what follows it. Thus, we're gonna optimize this procedure by first finding all of the pairs of words that follow each other, and then using this data to create sentences. Let's go through each of these steps in more detail:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Generate a data structure of word frequencies\n",
    "\n",
    "This is actually the harder part of the problem!\n",
    "\n",
    "Our goals here are to\n",
    "1. Read a file\n",
    "2. Use this file to populate our dictionary of words, and their follwing words\n",
    "3. Normalize the word counts into probabilities\n",
    "\n",
    "Let's talk about what this data structure will look like, and how we'll generate it. For this example, our text will consist soleley of the sentence \n",
    "\n",
    "```python\n",
    "\"I do what I think I want to do.\"\n",
    "```\n",
    "\n",
    "The unique words in this sentence are \"I\", 'do\", \"what\", \"want\", \"to\", \"think\". Our main data structure will be a dictionary where the keys are these unqiue words. The values will be the word counts for the words that follow our unique word in the sentence. So the first few entries in our dictionary will look like\n",
    "\n",
    "```python\n",
    "word_dict = {\n",
    "    \"I\": {\"do\": 1, \"think\": 1, \"want\": 1},\n",
    "    \"do\": {\"what\": 1},\n",
    "    \"what\": {\"I\": 1},\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "We're going to try to decompose this problem into a series of functions. Your job will be to fill in the bodies of these functions.\n",
    "\n",
    "## Extract pairs of words from a string\n",
    "\n",
    "One of the nice things about Markov chains is that we don't need to know anything about the context of a sentence as we will out this data structure. In fact, all of the information we need to fill this data structure is encoded as pairs of words. \n",
    "\n",
    "Let's start by writing a function that can extract pairs of words from a string. The string may contain multiple sentences - to generate more realistic text, we won't remove any punctation or capitalization from these words. That way, we'll actually learn the words that are likely to end sentences, and the words that are likely to begin the subsequent sentence!\n",
    "\n",
    "We'll return these words as a list of two word tuples, where the first item is the first word, and the second item is the following word. There will be no pair for the last word of the string (we'll ignore it for now.) Using our same sentence, this will produce\n",
    "\n",
    "```python\n",
    "[('I', 'do'), ('do', 'what'),\n",
    " ('what', 'I'), ('I', 'think'),\n",
    " ('think', 'I'), ('I', 'want'),\n",
    " ('want', 'to'), ('to', 'do.')]\n",
    "```\n",
    "\n",
    "In computational linguistics, these pairs of words are called digraphs (di- meaning 2, -graph meaning word.) We'll call our function split_line_to_digraphs()\n",
    "\n",
    "If the string contains fewer than 2 words, you should return an empty list []. Make sure to handle this case!\n",
    "\n",
    "If you need some help writing this function, it might be helpful to look at the \"largest number\" example in data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_line_to_digraphs(line):\n",
    "    \"\"\"\n",
    "    Given a string of words, returns a list of tuples \n",
    "    containing digraphs from the sentence\n",
    "    \"\"\"\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_digraphs = split_line_to_digraphs(\"I do what I think I want to do.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_blink_182 (__main__.DigraphTest) ... ok\n",
      "test_dr_seuss (__main__.DigraphTest) ... ok\n",
      "test_single_word (__main__.DigraphTest) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.004s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=3 errors=0 failures=0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seuss_output = [('I', 'do'), \n",
    " ('do', 'not'),\n",
    " ('not', 'like'),\n",
    " ('like', 'green'),\n",
    " ('green', 'eggs'),\n",
    " ('eggs', 'and'),\n",
    " ('and', 'Ham.')]\n",
    "\n",
    "blink_182_output = [('All', 'the,'),\n",
    " ('the,', 'Small'),\n",
    " ('Small', 'things,'),\n",
    " ('things,', 'True'),\n",
    " ('True', 'care,'),\n",
    " ('care,', 'Truth'),\n",
    " ('Truth', 'brings')]\n",
    "\n",
    "\n",
    "class DigraphTest(unittest.TestCase):\n",
    "    def test_dr_seuss(self):\n",
    "        self.assertEqual(split_line_to_digraphs(\"I do not like green eggs and Ham.\"), seuss_output)\n",
    "    def test_blink_182(self):\n",
    "        self.assertEqual(split_line_to_digraphs(\"All the, Small things, True care, Truth brings\"), blink_182_output)\n",
    "    def test_single_word(self):\n",
    "        self.assertEqual(split_line_to_digraphs('word'), [])\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(DigraphTest)\n",
    "unittest.TextTestRunner(verbosity=2).run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the digraph lists to a dictionary structure\n",
    "\n",
    "Our next step is to transform this list of digraphs into a nested dictionary structure with words counts, like the structure we described above (the example with word_dict.)\n",
    "\n",
    "This function will be a little tricky, because it involves keeping track of dictionaries nested within another dictionary. Take it slowly, and think carefully about the intermediate steps of the function. If you need to, you can try putting print() statements in the function at intermediate points to see how the variables look.\n",
    "\n",
    "For our sample sentence\n",
    "\n",
    "```python\n",
    "\"I do what I think I want to do\"\n",
    "```\n",
    "\n",
    "The output will be \n",
    "\n",
    "```python\n",
    "{'I': {'do': 1, 'think': 1, 'want': 1},\n",
    " 'do': {'what': 1},\n",
    " 'think': {'I': 1},\n",
    " 'to': {'do.': 1},\n",
    " 'want': {'to': 1},\n",
    " 'what': {'I': 1}}\n",
    "```\n",
    "\n",
    "A few things to keep in mind:\n",
    "1. Make sure that you test whether the first word in the digraph is already one of the unique words in the outer dictionary\n",
    "2. Make sure to test if the second word is already in the inner dictionary. If it isn't - think about how you would add it and what its value would be\n",
    "\n",
    "Remember that to add one to an existing value in python, you can use the shorthand \n",
    "\n",
    "```python\n",
    "value += 1 \n",
    "```\n",
    "\n",
    "instead of \n",
    "\n",
    "```python\n",
    "value = value + 1 \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_digraphs_to_dict(digraphs):\n",
    "    return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_seuss_sentence (__main__.GraphTests) ... FAIL\n",
      "test_test_sentence (__main__.GraphTests) ... FAIL\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_seuss_sentence (__main__.GraphTests)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-79-7c028ca3d7fa>\", line 22, in test_seuss_sentence\n",
      "    self.assertEqual(convert_digraphs_to_dict(seuss_output), seuss_dict)\n",
      "AssertionError: {} != {'like': {'green': 1}, 'green': {'eggs': 1[91 chars]: 1}}\n",
      "- {}\n",
      "+ {'I': {'do': 1},\n",
      "+  'and': {'Ham.': 1},\n",
      "+  'do': {'not': 1},\n",
      "+  'eggs': {'and': 1},\n",
      "+  'green': {'eggs': 1},\n",
      "+  'like': {'green': 1},\n",
      "+  'not': {'like': 1}}\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_test_sentence (__main__.GraphTests)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-79-7c028ca3d7fa>\", line 24, in test_test_sentence\n",
      "    self.assertEqual(convert_digraphs_to_dict(test_digraphs), test_sentence_dict)\n",
      "AssertionError: {} != {'want': {'to': 1}, 'to': {'do.': 1}, 'thi[85 chars]: 1}}\n",
      "- {}\n",
      "+ {'I': {'do': 1, 'think': 1, 'want': 1},\n",
      "+  'do': {'what': 1},\n",
      "+  'think': {'I': 1},\n",
      "+  'to': {'do.': 1},\n",
      "+  'want': {'to': 1},\n",
      "+  'what': {'I': 1}}\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.021s\n",
      "\n",
      "FAILED (failures=2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=2 errors=0 failures=2>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seuss_dict = {\n",
    "     'I': {'do': 1},\n",
    "     'and': {'Ham.': 1},\n",
    "     'do': {'not': 1},\n",
    "     'eggs': {'and': 1},\n",
    "     'green': {'eggs': 1},\n",
    "     'like': {'green': 1},\n",
    "     'not': {'like': 1}\n",
    "}\n",
    "\n",
    "test_sentence_dict = {\n",
    "    'I': {'do': 1, 'think': 1, 'want': 1},\n",
    "    'do': {'what': 1},\n",
    "    'think': {'I': 1},\n",
    "    'to': {'do.': 1},\n",
    "    'want': {'to': 1},\n",
    "    'what': {'I': 1}\n",
    "}\n",
    "\n",
    "class GraphTests(unittest.TestCase):\n",
    "    def test_seuss_sentence(self):\n",
    "        self.assertEqual(convert_digraphs_to_dict(seuss_output), seuss_dict)\n",
    "    def test_test_sentence(self):\n",
    "        self.assertEqual(convert_digraphs_to_dict(test_digraphs), test_sentence_dict)\n",
    "        \n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(GraphTests)\n",
    "unittest.TextTestRunner(verbosity=2).run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allow for updating the structure with new data\n",
    "\n",
    "We likely will not be processing our file of interest as one big line. Instead, we'll go through it line by line, and update a main dictionary with the data from each line. We've already written functions to turn a string into digraphs, and convert those digraphs into a word frequency dictionary. Our next next is to write a function that can merge two frequency dictionaries together. \n",
    "\n",
    "Basically, we want to go word by word through each dictionary. When the dictionaries have a word in common, we want to merge their frequency counts (the inner dictionaries) for the following word, so that\n",
    "1. If the word only appears in one frequency dictionary, it keeps its count\n",
    "2. If the word appears in both frequency dictionaries, its count becomes the sum of the two counts\n",
    "If the word in the outer dictionary is not common, we just want to add it to the merged dictionary as it is\n",
    "\n",
    "If this merge process for the inner dictionaries seems familiar, it's because you already solved it in the data structures lesson! As coders, we want to be as lazy as possible. The best code is the code that you (or someone else) already wrote! Let's copy your solution below, and then write a short function to handle the merge of the outer dictionaries together. If you don't have your solution for the inner dictionary merge, don't worry. We've provided a sample solution below.\n",
    "\n",
    "Here's sample inputs for the merge_outer_dictionaries function, and the expected output\n",
    "\n",
    "```python\n",
    "input_1 = {'I': {'do': 1},\n",
    " 'and': {'Ham.': 1},\n",
    " 'do': {'not': 1},\n",
    " 'eggs': {'and': 1},\n",
    " 'green': {'eggs': 1},\n",
    " 'like': {'green': 1},\n",
    " 'not': {'like': 1}}\n",
    "\n",
    "input_2 = {'I': {'do': 1, 'think': 1, 'want': 1},\n",
    " 'do': {'what': 1},\n",
    " 'think': {'I': 1},\n",
    " 'to': {'do.': 1},\n",
    " 'want': {'to': 1},\n",
    " 'what': {'I': 1}}\n",
    " \n",
    "merge_outer_dictionaries(input_1, input_2)\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```python\n",
    "{'I': {'do': 2, 'think': 1, 'want': 1},\n",
    " 'and': {'Ham.': 1},\n",
    " 'do': {'not': 1, 'what': 1},\n",
    " 'eggs': {'and': 1},\n",
    " 'green': {'eggs': 1},\n",
    " 'like': {'green': 1},\n",
    " 'not': {'like': 1},\n",
    " 'think': {'I': 1},\n",
    " 'to': {'do.': 1},\n",
    " 'want': {'to': 1},\n",
    " 'what': {'I': 1}}\n",
    "```\n",
    "\n",
    "If you get stuck, the solution for the outer dictionaries is similar, but not identical to the similar for the inner dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def merge_and_sum_dictionaries(counts_1, counts_2):\n",
    "    combined_counts = {} \n",
    "    \n",
    "    # Copy contents of first dictionary to combined counts\n",
    "    for word, count in counts_1.items():\n",
    "        combined_counts[word] = count\n",
    "    \n",
    "    for word, count in counts_2.items():\n",
    "        if word in combined_counts:\n",
    "            combined_counts[word] += count\n",
    "        else:\n",
    "            combined_counts[word] = count\n",
    "    return combined_counts\n",
    "\n",
    "def merge_outer_dictionaries(old_words, new_words):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    merged_words = {}\n",
    "    return merged_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_merge (__main__.MergeTest) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = {'I': {'do': 2, 'think': 1, 'want': 1},\n",
    " 'and': {'Ham.': 1},\n",
    " 'do': {'not': 1, 'what': 1},\n",
    " 'eggs': {'and': 1},\n",
    " 'green': {'eggs': 1},\n",
    " 'like': {'green': 1},\n",
    " 'not': {'like': 1},\n",
    " 'think': {'I': 1},\n",
    " 'to': {'do.': 1},\n",
    " 'want': {'to': 1},\n",
    " 'what': {'I': 1}}\n",
    "\n",
    "class MergeTest(unittest.TestCase):\n",
    "    def test_merge(self):\n",
    "        self.assertEqual(merge_outer_dictionaries(seuss_dict, test_sentence_dict), output)\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(MergeTest)\n",
    "unittest.TextTestRunner(verbosity=2).run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize a count dictionary\n",
    "\n",
    "One last thing: after we've counted all of the words in the text, we want to normalize the counts to get the probability of all the different words that might follow our word of interest. Let's write a simple function to take a dictionary of counts for string values, and normalize the counts by dividing each one by the sum of all of the counts.\n",
    "\n",
    "A pair of expected input and output for this function are:\n",
    "\n",
    "```python\n",
    "words = {'Hello': 2, 'World':3}\n",
    "normalize_count(words)\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```python\n",
    "{'Hello': 0.4, 'World': 0.6}\n",
    "```\n",
    "\n",
    "One hint: Python has a built in sum() function that will calculate the sum of a list of numbers. Do you know how to get a list of all the count values of a count dictionary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_counts(count_dictionary):\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_four_words (__main__.NormalizationTest) ... ok\n",
      "test_two_words (__main__.NormalizationTest) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.018s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=2 errors=0 failures=0>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NormalizationTest(unittest.TestCase):\n",
    "    def test_two_words(self):\n",
    "        self.assertEqual(normalize_counts({'Hello': 2, 'World':3}), {'Hello': 0.4, 'World': 0.6})\n",
    "    def test_four_words(self):\n",
    "        self.assertEqual(normalize_counts({'These': 2, 'Are':3, 'Seperate': 5, 'Words':10}),\n",
    "                         {'These': 0.1, 'Are':0.15, 'Seperate': 0.25, 'Words':0.5})\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(NormalizationTest)\n",
    "unittest.TextTestRunner(verbosity=2).run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the word data from a file, and generate a dictionary of words\n",
    "\n",
    "For this function, you will take the name of a file as an input, and return a dictionary with the full normalized counts for the entire text of the file. Make sure not to normalize until you have all of the counts!\n",
    "\n",
    "If you think you know how to combine the functions above to do this, you're welcome to try! Otherwise you can follow the following outline\n",
    "\n",
    "1. Create an output dictionary to hold all of the word counts\n",
    "2. Open the file\n",
    "3. For every line in the file\n",
    "    1. Split the line into digraphs\n",
    "    2. Use the digraphs to create a count dictionary for the line\n",
    "    3. Update the output dictionary's counts with the line dictionary\n",
    "4. For every entry in the output dictionary\n",
    "    1. Normalize the entry\n",
    "5. Return the output dictionary\n",
    "\n",
    "This function will rely on all of the functions that you wrote above. If one of your implementations didn't work, feel free to use the function from the rel_impl library. Instead of calling \n",
    "```python\n",
    "merge_outer_dictionaries()\n",
    "```\n",
    "you would call \n",
    "```python\n",
    "rel_impl.merge_outer_dictionaries()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word_counts_from_file(filename):\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I': {'do': 0.5, 'think': 0.25, 'want': 0.25},\n",
       " 'and': {'Ham.': 1.0},\n",
       " 'do': {'not': 0.5, 'what': 0.5},\n",
       " 'eggs': {'and': 1.0},\n",
       " 'green': {'eggs': 1.0},\n",
       " 'like': {'green': 1.0},\n",
       " 'not': {'like': 1.0},\n",
       " 'think': {'I': 1.0},\n",
       " 'to': {'do.': 1.0},\n",
       " 'want': {'to': 1.0},\n",
       " 'what': {'I': 1.0}}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_impl.word_counts_from_file('text_corpuses/small.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_small_corpus (__main__.ReadFileTest) ... FAIL\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_small_corpus (__main__.ReadFileTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-94-f9dd4f70cfb6>\", line 18, in test_small_corpus\n",
      "    self.assertEqual(output, sample_output)\n",
      "AssertionError: {} != {'want': {'to': 1.0}, 'to': {'do.': 1.0}, [229 chars]1.0}}\n",
      "- {}\n",
      "+ {'I': {'do': 0.5, 'think': 0.25, 'want': 0.25},\n",
      "+  'and': {'Ham.': 1.0},\n",
      "+  'do': {'not': 0.5, 'what': 0.5},\n",
      "+  'eggs': {'and': 1.0},\n",
      "+  'green': {'eggs': 1.0},\n",
      "+  'like': {'green': 1.0},\n",
      "+  'not': {'like': 1.0},\n",
      "+  'think': {'I': 1.0},\n",
      "+  'to': {'do.': 1.0},\n",
      "+  'want': {'to': 1.0},\n",
      "+  'what': {'I': 1.0}}\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.004s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=1>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_output = {\n",
    "    'I': {'do': 0.5, 'think': 0.25, 'want': 0.25},\n",
    "    'and': {'Ham.': 1.0},\n",
    "    'do': {'not': 0.5, 'what': 0.5},\n",
    "    'eggs': {'and': 1.0},\n",
    "    'green': {'eggs': 1.0},\n",
    "    'like': {'green': 1.0},\n",
    "    'not': {'like': 1.0},\n",
    "    'think': {'I': 1.0},\n",
    "    'to': {'do.': 1.0},\n",
    "    'want': {'to': 1.0},\n",
    "    'what': {'I': 1.0}\n",
    "}\n",
    "\n",
    "class ReadFileTest(unittest.TestCase):\n",
    "    def test_small_corpus(self):\n",
    "        output = word_counts_from_file('text_corpuses/small.txt')\n",
    "        self.assertEqual(output, sample_output)\n",
    "    \n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(ReadFileTest)\n",
    "unittest.TextTestRunner(verbosity=2).run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load speech data\n",
    "\n",
    "Using the functions you've defined above, we're now going to load in the data from the Trump and Obama speeches that we have saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trump_counts = word_counts_from_file('text_corpuses/trump_speeches.txt')\n",
    "obama_counts = word_counts_from_file('text_corpuses/obama_speeches.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate random sentences\n",
    "\n",
    "Now for the easy/rewarding part! With this data of the most frequent words that follow each word, we can easily generate some fake sentences!\n",
    "\n",
    "Let's create a function that will take a count dictionary and a number of words as inputs, and output a sentence based on the word frequencies. To do this, we'll need one capability that isn't available in base python: we need to be able to generate random numbers, to select random words from the dictionary. To do this, we'll import the *random* module below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the random module has a function called random.random(), which generates a random number between 0 and 1. Try running the line below a couple of times - you'll see that it changes values every time you run it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5184939298387877"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's also a function in the random module called random.choice, which randomly picks an element from a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(['a', 'b', 'c', 'd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select a random word using a weighted average\n",
    "\n",
    "We're gonna use the following algorithm to select a random word with the probability that it would normally occur in the text.\n",
    "\n",
    "There's probably a few good ways to do this. I thought about it for a little and came up with the following algorithm, but it's probably not the most efficient/best way to do it. If you have a better idea, email me!\n",
    "\n",
    "1. Generate a random number between zero and 1 - let's call this the cutoff probability\n",
    "2. Create a variable to store a running total probability\n",
    "3. Iterate through the possible words and their probabilities. For each word, add the individual probability of that word to the running total probability.\n",
    "4. When the running total probability becomes higher than the cutoff probability, return the current word\n",
    "5. If this doesn't happen, return the last word that we iterated over\n",
    "\n",
    "Note that since this is a random function, the test that we're running just tests to make sure that the output is reasonably close to the expected outcome. There's always a strong possibility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_choice_from_frequency_dict(frequency_dict):\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_within_boundaries (__main__.TestRandomness) ... ERROR\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_within_boundaries (__main__.TestRandomness)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-108-db56fc9622c0>\", line 16, in test_within_boundaries\n",
      "    for word in frequencies)\n",
      "  File \"<ipython-input-108-db56fc9622c0>\", line 16, in <genexpr>\n",
      "    for word in frequencies)\n",
      "KeyError: 'These'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.029s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=1 failures=0>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies = {'These': 0.1, 'Are':0.15, 'Seperate': 0.25, 'Words':0.5}\n",
    "\n",
    "class TestRandomness(unittest.TestCase):\n",
    "    def test_within_boundaries(self):\n",
    "        counts = {}\n",
    "        for i in range(10000):\n",
    "            word = random_choice_from_frequency_dict(frequencies)\n",
    "            if word not in counts:\n",
    "                counts[word] = 1\n",
    "            else:\n",
    "                counts[word] += 1\n",
    "        ideal_values = {word: int(freq * 10000) \n",
    "                        for word, freq \n",
    "                        in frequencies.items()}\n",
    "        max_deviation = max(abs(counts[word] - ideal_values[word]) \n",
    "                            for word in frequencies)\n",
    "        error_message = 'Your counts deviated too much from the ideal values!\\nYour counts:{}, ideal counts:{}'\n",
    "        error_message = error_message.format(counts, ideal_values)\n",
    "        self.assertLess(max_deviation, 100, error_message)\n",
    "        \n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestRandomness)\n",
    "unittest.TextTestRunner(verbosity=2).run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally it's time to use the data structure we've created and this random word function to generate a sentence. Can you do it? Here's a sketch of how your function will work:\n",
    "\n",
    "1. Make an list to hold the words in the sentence \n",
    "2. Pick a random word from the data structure's keys. Store it as current_word and add it to the list.\n",
    "3. Loop over the number of words we want to generate\n",
    "    - Use the current_word, the data structure and our random_choice_from_frequency_dictionary() function to pick the next word\n",
    "    - Add the next word to the list\n",
    "    - Set the next word as the current_word for the next loop\n",
    "4. Return the list, joined with spaces into a sentence\n",
    "\n",
    "Two hints that might be useful for this function\n",
    "1. To pick a random element from a list (for the starting word) you can use the random.choice() function\n",
    "2. To do a loop a certain number of times, you can use a for loop with the range() function. We'll talk more about range() in the future, but for now you can treat range(n) as returning a list of integers from 0 to n. So to do something 50 times, you would do:\n",
    "```python\n",
    "for i in range(50):\n",
    "    do_something()\n",
    "    # i is an \"index\" variable so it will hold the current count\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_sentence(word_dict, number_words):\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lowered, they had a lot of them, \"We canâ€™t go on the way, I HAVE THE BEGINNING, AND I\\'LL TELL YOU ARE BACK THE COVER -- the single dollar can keep getting ready for somebody slipped â€” \"Oh, you know this election thatâ€™s the worst trade deals like that. Weâ€™re going to tell them in the table. Say a journey and in certain people are coming in big investments in, but, guess six, seven, eight or our soil. He said 49% to be very exciting. We canâ€™t get any more. But â€™17 â€“ and itâ€™s $2.5 billion and people away.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_impl.generate_sentence(trump_counts, number_words=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"discussion, at the Administration's strategy cannot live in this front. For the site of the White House will become Americans, this amnesty there were out there will be held a national commitment necessary to us, and many things. They are different result. This is right, in 1899 and torture in the right path, they don't think many times. But I hear about religious people. I realize that may not wasting one of not to be otherwise is about. Not just revolve around their kids can't even a man's prerogative, it takes to beef up their own science and months ahead.\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence(obama_counts, number_words=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
