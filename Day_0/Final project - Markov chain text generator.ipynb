{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congrats! ðŸŽ‰\n",
    "\n",
    "Believe it or not, you've learned a huge chunk of the Python language, and have all the basic skills you need to move on to learning the scientific libraries and start playing with data! We'll start doing this in the next set of excercises. There's one chapter after this one that you should complete, which will deal with some more advanced language features that are extremely useful, and make Python a more fun and expressive language to code in. For now though, you should feel accomplished that you've made it this far!\n",
    "\n",
    "The concepts and skills that you've learned (the if statement, for loops, lists, dictionaries, strings) are pretty universal among programming languages. If you wanted to try web programming in JavaScript, or write Desktop applications in C++, you'll find these same concepts come up.\n",
    "\n",
    "What we're gonna do now is try to put together all of the basic Python you've learned into one coherent project. This way, you can get a feel for how all of these parts work together beyond the single-function excercises that you've been doing.\n",
    "\n",
    "# Our project\n",
    "\n",
    "What we're going to create is a Markov-Chain text generator. In effect, what this program is going to do is read a file with sentences from a particular source (a novel, song lyrics, speeches, etc.) and then generate random, usually nonsensical text in the style of this source material. The algorithm to do this is incredibly simple, but these programs can sometimes create quite sophisticated looking sentences! \n",
    "\n",
    "If you want an example of what Markov Chains can do, the website Reddit has a community (http://www.reddit.com/r/SubredditSimulator) where the title of every post, as well as every comment on the posts, are generated randomly from the text of other communities on Reddit. The results are often pretty funny, and sometimes seem surprisingly intelligent.\n",
    "\n",
    "We're gonna quickly go through what a Markov Chain is, how we can use it to generate text, and then talk about our strategy for implementing one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill in details of what a Markov Chain is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill in details of the implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Generate a data structure of word frequencies\n",
    "\n",
    "This is actually the harder part of the problem!\n",
    "\n",
    "Our goals here are to\n",
    "1. Read a file\n",
    "2. Use this file to populate our dictionary of words, and their follwing words\n",
    "3. Normalize the word counts into probabilities\n",
    "\n",
    "Let's talk about what this data structure will look like, and how we'll generate it. For this example, our text will consist soleley of the sentence \n",
    "\n",
    "```python\n",
    "\"I do what I think I want to do.\"\n",
    "```\n",
    "\n",
    "The unique words in this sentence are \"I\", 'do\", \"what\", \"want\", \"to\", \"think\". Our main data structure will be a dictionary where the keys are these unqiue words. The values will be the word counts for the words that follow our unique word in the sentence. So the first few entries in our dictionary will look like\n",
    "\n",
    "```python\n",
    "word_dict = {\n",
    "    \"I\": {\"do\": 1, \"think\": 1, \"want\": 1},\n",
    "    \"do\": {\"what\": 1},\n",
    "    \"what\": {\"I\": 1},\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "We're going to try to decompose this problem into a series of functions. Your job will be to fill in the bodies of these functions.\n",
    "\n",
    "## Extract pairs of words from a string\n",
    "\n",
    "One of the nice things about Markov chains is that we don't need to know anything about the context of a sentence as we will out this data structure. In fact, all of the information we need to fill this data structure is encoded as pairs of words. \n",
    "\n",
    "Let's start by writing a function that can extract pairs of words from a string. The string may contain multiple sentences - to generate more realistic text, we won't remove any punctation or capitalization from these words. That way, we'll actually learn the words that are likely to end sentences, and the words that are likely to begin the subsequent sentence!\n",
    "\n",
    "We'll return these words as a list of two word tuples, where the first item is the first word, and the second item is the following word. There will be no pair for the last word of the string (we'll ignore it for now.) Using our same sentence, this will produce\n",
    "\n",
    "```python\n",
    "[('I', 'do'), ('do', 'what'),\n",
    " ('what', 'I'), ('I', 'think'),\n",
    " ('think', 'I'), ('I', 'want'),\n",
    " ('want', 'to'), ('to', 'do.')]\n",
    "```\n",
    "\n",
    "In computational linguistics, these pairs of words are called digraphs (di- meaning 2, -graph meaning word.) We'll call our function split_line_to_digraphs()\n",
    "\n",
    "If the string contains fewer than 2 words, you should return an empty list []. Make sure to handle this case!\n",
    "\n",
    "If you need some help writing this function, it might be helpful to look at the \"largest number\" example in data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_line_to_digraphs(line):\n",
    "    \"\"\"\n",
    "    Given a string of words, returns a list of tuples \n",
    "    containing digraphs from the sentence\n",
    "    \"\"\"\n",
    "    return []\n",
    "\n",
    "def split_line_to_digraphs(line):\n",
    "    words = line.split()\n",
    "    \n",
    "    # Ignore strings with fewer than two words\n",
    "    if len(words) < 2:\n",
    "        return []\n",
    "    \n",
    "    digraphs = []\n",
    "    \n",
    "    current_word = words[0]\n",
    "    for next_word in words[1:]:\n",
    "        digraphs.append((current_word, next_word))\n",
    "        current_word = next_word\n",
    "        \n",
    "    return digraphs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_digraphs = split_line_to_digraphs(\"I do what I think I want to do.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seuss_output = [('I', 'do'), \n",
    " ('do', 'not'),\n",
    " ('not', 'like'),\n",
    " ('like', 'green'),\n",
    " ('green', 'eggs'),\n",
    " ('eggs', 'and'),\n",
    " ('and', 'Ham.')]\n",
    "\n",
    "blink_182_output = [('All', 'the,'),\n",
    " ('the,', 'Small'),\n",
    " ('Small', 'things,'),\n",
    " ('things,', 'True'),\n",
    " ('True', 'care,'),\n",
    " ('care,', 'Truth'),\n",
    " ('Truth', 'brings')]\n",
    "\n",
    "assert split_line_to_digraphs(\"I do not like green eggs and Ham.\") == seuss_output\n",
    "assert split_line_to_digraphs(\"Hello\") == []\n",
    "assert split_line_to_digraphs(\"All the, Small things, True care, Truth brings\") == blink_182_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the digraph lists to a dictionary structure\n",
    "\n",
    "Our next step is to transform this list of digraphs into a nested dictionary structure with words counts, like the structure we described above (the example with word_dict.)\n",
    "\n",
    "This function will be a little tricky, because it involves keeping track of dictionaries nested within another dictionary. Take it slowly, and think carefully about the intermediate steps of the function. If you need to, you can try putting print() statements in the function at intermediate points to see how the variables look.\n",
    "\n",
    "For our sample sentence\n",
    "\n",
    "```python\n",
    "\"I do what I think I want to do\"\n",
    "```\n",
    "\n",
    "The output will be \n",
    "\n",
    "```python\n",
    "{'I': {'do': 1, 'think': 1, 'want': 1},\n",
    " 'do': {'what': 1},\n",
    " 'think': {'I': 1},\n",
    " 'to': {'do.': 1},\n",
    " 'want': {'to': 1},\n",
    " 'what': {'I': 1}}\n",
    "```\n",
    "\n",
    "A few things to keep in mind:\n",
    "1. Make sure that you test whether the first word in the digraph is already one of the unique words in the outer dictionary\n",
    "2. Make sure to test if the second word is already in the inner dictionary. If it isn't - think about how you would add it and what its value would be\n",
    "\n",
    "Remember that to add one to an existing value in python, you can use the shorthand \n",
    "\n",
    "```python\n",
    "value += 1 \n",
    "```\n",
    "\n",
    "instead of \n",
    "\n",
    "```python\n",
    "value = value + 1 \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_digraphs_to_dict(digraphs):\n",
    "    return {}\n",
    "\n",
    "def convert_digraphs_to_dict(digraphs):\n",
    "    words_with_frequencies = {}\n",
    "    for first_word, second_word in digraphs:\n",
    "        if first_word not in words_with_frequencies:\n",
    "            words_with_frequencies[first_word] = {second_word: 1}\n",
    "        elif second_word not in words_with_frequencies[first_word]:\n",
    "            words_with_frequencies[first_word][second_word] = 1\n",
    "        else:\n",
    "            words_with_frequencies[first_word][second_word] += 1\n",
    "    return words_with_frequencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I': {'do': 1, 'think': 1, 'want': 1},\n",
       " 'do': {'what': 1},\n",
       " 'think': {'I': 1},\n",
       " 'to': {'do.': 1},\n",
       " 'want': {'to': 1},\n",
       " 'what': {'I': 1}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_digraphs_to_dict(test_digraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allow for updating the structure with new data\n",
    "\n",
    "We likely will not be processing our file of interest as one big line. Instead, we'll go through it line by line, and update a main dictionary with the data from each line. We've already written functions to turn a string into digraphs, and convert those digraphs into a word frequency dictionary. Our next next is to write a function that can merge two frequency dictionaries together. \n",
    "\n",
    "Basically, we want to go word by word through each dictionary. When the dictionaries have a word in common, we want to merge their frequency counts (the inner dictionaries) for the following word, so that\n",
    "1. If the word only appears in one frequency dictionary, it keeps its count\n",
    "2. If the word appears in both frequency dictionaries, its count becomes the sum of the two counts\n",
    "If the word in the outer dictionary is not common, we just want to add it to the merged dictionary as it is\n",
    "\n",
    "If this merge process for the inner dictionaries seems familiar, it's because you already solved it in the data structures lesson! As coders, we want to be as lazy as possible. The best code is the code that you (or someone else) already wrote! Let's copy your solution below, and then write a short function to handle the merge of the outer dictionaries together. If you don't have your solution for the inner dictionary merge, don't worry. We've provided a sample solution below.\n",
    "\n",
    "Here's sample inputs for the merge_outer_dictionaries function, and the expected output\n",
    "\n",
    "```python\n",
    "input_1 = {'I': {'do': 1},\n",
    " 'and': {'Ham.': 1},\n",
    " 'do': {'not': 1},\n",
    " 'eggs': {'and': 1},\n",
    " 'green': {'eggs': 1},\n",
    " 'like': {'green': 1},\n",
    " 'not': {'like': 1}}\n",
    "\n",
    "input_2 = {'I': {'do': 1, 'think': 1, 'want': 1},\n",
    " 'do': {'what': 1},\n",
    " 'think': {'I': 1},\n",
    " 'to': {'do.': 1},\n",
    " 'want': {'to': 1},\n",
    " 'what': {'I': 1}}\n",
    " \n",
    "merge_outer_dictionaries(input_1, input_2)\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```python\n",
    "{'I': {'do': 2, 'think': 1, 'want': 1},\n",
    " 'and': {'Ham.': 1},\n",
    " 'do': {'not': 1, 'what': 1},\n",
    " 'eggs': {'and': 1},\n",
    " 'green': {'eggs': 1},\n",
    " 'like': {'green': 1},\n",
    " 'not': {'like': 1},\n",
    " 'think': {'I': 1},\n",
    " 'to': {'do.': 1},\n",
    " 'want': {'to': 1},\n",
    " 'what': {'I': 1}}\n",
    "```\n",
    "\n",
    "If you get stuck, the solution for the outer dictionaries is similar, but not identical to the similar for the inner dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def merge_and_sum_dictionaries(counts_1, counts_2):\n",
    "    combined_counts = {} \n",
    "    \n",
    "    # Copy contents of first dictionary to combined counts\n",
    "    for word, count in counts_1.items():\n",
    "        combined_counts[word] = count\n",
    "    \n",
    "    for word, count in counts_2.items():\n",
    "        if word in combined_counts:\n",
    "            combined_counts[word] += count\n",
    "        else:\n",
    "            combined_counts[word] = count\n",
    "    return combined_counts\n",
    "\n",
    "def merge_outer_dictionaries(old_words, new_words):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    merged_words = {}\n",
    "    return merged_words\n",
    "\n",
    "def merge_outer_dictionaries(old_words, new_words):\n",
    "    merged_words = {}\n",
    "    for word, counts_dict in old_words.items():\n",
    "        merged_words[word] = counts_dict\n",
    "    for word, counts_dict in new_words.items():\n",
    "        if word in merged_words:\n",
    "            merged_words[word] = merge_and_sum_dictionaries(merged_words[word], counts_dict)\n",
    "        else:\n",
    "            merged_words[word] = counts_dict\n",
    "    return merged_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize a count dictionary\n",
    "\n",
    "One last thing: after we've counted all of the words in the text, we want to normalize the counts to get the probability of all the different words that might follow our word of interest. Let's write a simple function to take a dictionary of counts for string values, and normalize the counts by dividing each one by the sum of all of the counts.\n",
    "\n",
    "A pair of expected input and output for this function are:\n",
    "\n",
    "```python\n",
    "words = {'Hello': 2, 'World':3}\n",
    "normalize_count(words)\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```python\n",
    "{'Hello': 0.4, 'World': 0.6}\n",
    "```\n",
    "\n",
    "One hint: Python has a built in sum() function that will calculate the sum of a list of numbers. Do you know how to get a list of all the count values of a count dictionary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_counts(count_dictionary):\n",
    "    return {}\n",
    "\n",
    "def normalize_counts(count_dictionary):\n",
    "    total = sum(count_dictionary.values())\n",
    "    output_dict = {}\n",
    "    for word, count in count_dictionary.items():\n",
    "        output_dict[word] = count/total\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert normalize_counts({'Hello': 2, 'World':3}) == {'Hello': 0.4, 'World': 0.6}\n",
    "assert normalize_counts({'These': 2, 'Are':3, 'Seperate': 5, 'Words':10}) == {'These': 0.1, 'Are':0.15, 'Seperate': 0.25, 'Words':0.5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the word data from a file, and generate a dictionary of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('text_corpuses/trump_speeches.txt') as speech_file:\n",
    "    word_counts = {}\n",
    "    for line in speech_file:\n",
    "        digraphs = split_line_to_digraphs(line)\n",
    "        line_counts = convert_digraphs_to_dict(digraphs)\n",
    "        word_counts = merge_outer_dictionaries(word_counts, line_counts)\n",
    "    for word, counts in word_counts.items():\n",
    "        word_counts[word] = normalize_counts(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate random sentences\n",
    "\n",
    "Now for the easy/rewarding part! With this data of the most frequent words that follow each word, we can easily generate some fake sentences!\n",
    "\n",
    "Let's create a function that will take a count dictionary and a number of words as inputs, and output a sentence based on the word frequencies. To do this, we'll need one capability that isn't available in base python: we need to be able to generate random numbers, to select random words from the dictionary. To do this, we'll import the *random* module below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the random module has a function called random.random(), which generates a random number between 0 and 1. Try running the line below a couple of times - you'll see that it changes values every time you run it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8193624733995585"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's also a function in the random module called random.choice, which randomly picks an element from a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(['a', 'b', 'c', 'd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select a random word using a weighted average\n",
    "\n",
    "We're gonna use the following algorithm to select a random word with the probability that it would normally occur in the text.\n",
    "\n",
    "There's probably a few good ways to do this. I thought about it for a little and came up with the following algorithm, but it's probably not the most efficient/best way to do it. If you have a better idea, email me!\n",
    "\n",
    "1. Generate a random number between zero and 1 - let's call this the cutoff probability\n",
    "2. Create a variable to store a running total probability\n",
    "3. Iterate through the possible words and their probabilities. For each word, add the individual probability of that word to the running total probability.\n",
    "4. When the running total probability becomes higher than the cutoff probability, return the current word\n",
    "5. If this doesn't happen, return the last word that we iterated over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_choice_from_frequency_dict(frequency_dict):\n",
    "    probability_threshold = random.random()\n",
    "    total_probability = 0\n",
    "    for word, probability in frequency_dict.items():\n",
    "        total_probability += probability\n",
    "        if total_probability > probability_threshold:\n",
    "            return word\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_sentence(word_dict, number_words):\n",
    "    output_words = []\n",
    "    unique_words = list(word_dict.keys())\n",
    "    current_word = random.choice(unique_words)\n",
    "    for i in range(number_words):\n",
    "        output_words.append(current_word)\n",
    "        current_word = random_choice_from_frequency_dict(word_dict[current_word])\n",
    "    return \" \".join(output_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trades, by senators and trillions of them fight to get along and have trillions of Caterpillars. Caterpillarâ€™s stock market crashes. Iâ€™ve been able to keep Ben Carson, which is an ever done under budget that was brutal. I just call â€“ some of other things about the parents, who murder gays. I donâ€™t do it talked about it? You go over the history of a lot of it didnâ€™t bring education per pupil -- it had an amazing job, but we have a beautiful plane. Canâ€™t get hit them to make America building $2.5 trillion at zero. And we know'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence(word_counts, number_words=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
